# Elasticsearch Dummy Data Generator

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A simple yet powerful web application to generate and index large volumes of realistic sample data (web access logs or financial transactions) into your Elasticsearch cluster. Ideal for performance testing, demonstrations, or populating your development environment with meaningful data.

![image](https://github.com/user-attachments/assets/7f9e9908-7bb3-4527-90c7-074bd16268a9)


## Table of Contents
- [About The Project](#about-the-project)
- [Features](#features)
- [How It Works](#how-it-works)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Usage](#usage)
- [Generated Data Schemas](#generated-data-schemas)
  - [Transactions](#transactions)
  - [Access Logs](#access-logs)
- [Project Structure](#project-structure)

## About The Project

This project provides a web interface built with Flask and TailwindCSS to configure and launch a data generation task. The backend uses a Python script to generate structured fake data using the `Faker` library and efficiently indexes it into a specified Elasticsearch cluster using the bulk API.

The main goal is to remove the hassle of creating manual scripts to populate Elasticsearch, by providing a "plug-and-play" tool that anyone can run locally.

## Features

* **Intuitive Web Interface**: Configure and start the indexing process through a simple and clean UI.
* **Real-Time Logging**: Monitor the indexing progress directly in your browser with live-streamed logs from the server.
* **Two Data Types**: Choose between generating **Financial Transactions** or **Web Access Logs** data.
* **Flexible Configuration**: Specify the Elasticsearch cluster URL, credentials, target index name, and the total size of the data to be generated.
* **Realistic Data**: Data is generated by `Faker`, including names, IPs, geolocations, URLs, and more.
* **Smart Mappings**: The script automatically creates an optimized index mapping in Elasticsearch if the index doesn't exist.
* **Efficient Indexing**: Utilizes the Elasticsearch bulk API for high-performance data ingestion.
* **Entity Metadata**: Exports data for consistently generated entities (like merchants, hosts, and clients) to `.json` files for future reference.

## How It Works

The application consists of two main components working together:

1.  **Flask Web Application (`app.py`)**:
    * Serves the user interface (`index.html`).
    * Receives configuration parameters from the user via a form.
    * Launches the worker script (`script_executor.py`) as a subprocess, passing the configuration as command-line arguments.
    * Captures the script's standard output and streams it back to the browser to display logs in real-time.

2.  **Data Generator & Indexer (`script_executor.py`)**:
    * Connects to the specified Elasticsearch cluster.
    * Based on the selected `data type`, it generates documents in batches.
    * Ensures that entities like a merchant or a client always have the same attributes whenever they appear.
    * Sends the batches to Elasticsearch using the bulk API.
    * Logs its progress to the console, which is captured by the Flask app.

The flow is as follows:
`User (Browser) ➔ Flask Web UI (app.py) ➔ Subprocess (script_executor.py) ➔ Elasticsearch`

## Getting Started

Follow these instructions to get a copy of the project up and running on your local machine for development and testing purposes.

### Prerequisites

* Python 3.7+
* `pip` (Python package installer)
* An accessible Elasticsearch cluster.

### Installation

1.  **Clone the repository:**
    ```sh
    git clone [https://github.com/your-username/your-repository.git](https://github.com/your-username/your-repository.git)
    cd your-repository
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```sh
    python -m venv venv
    ```
    * On Windows:
        ```sh
        venv\Scripts\activate
        ```
    * On macOS/Linux:
        ```sh
        source venv/bin/activate
        ```

3.  **Install Python dependencies:**
    A `requirements.txt` file should be created with the content below.
    ```sh
    pip install -r requirements.txt
    ```
    **`requirements.txt` content:**
    ```
    Flask
    Faker
    elasticsearch
    ```

## Usage

1.  **Run the Flask application:**
    ```sh
    python app.py
    ```
    You will see output indicating the server is running, typically at `http://127.0.0.1:5000`.

2.  **Open the web interface:**
    Open your web browser and navigate to `http://127.0.0.1:5000`.

3.  **Fill out the form:**
    * **Elasticsearch URL and Port Number**: The full URL of your Elasticsearch instance (e.g., `https://your-host.es.cloud.ovh.net:9200` or `http://localhost:9200`).
    * **Username / Password**: Your Elasticsearch credentials.
    * **Target Index**: The name of the index where the data will be stored (e.g., `bank_transactions`).
    * **Total Data Size (MB)**: The approximate total size of the data you want to generate.
    * **Data Type**: Select "Transaction" or "Access Log".

4.  **Start Indexing:**
    Click the **"Start Data Indexing"** button.

5.  **Monitor the Logs:**
    Logs from the indexing process will appear in real-time in the "Log Output" area. You'll be able to see the progress, any warnings, or any errors that occur. A success message will appear upon completion.

## Generated Data Schemas

Below are examples of the documents that are generated for each data type.

### Transactions

Generates financial transaction data. After indexing, a `merchants.json` file will be created with the details of the generated merchants.

**Example Document (`transaction`):**
```json
{
    "merchant_id": "merchant-085",
    "@timestamp": "2024-03-11T14:22:01.123456",
    "account_number": "4921...-..-....",
    "transaction_amount": 750.50,
    "merchant": {
        "merchant_id": "merchant-085",
        "name": "Davis-Johnson",
        "type": "RETAIL",
        "location": { "lat": -34.5889, "lon": -58.3816 },
        "geo_info": {
            "city": "Buenos Aires",
            "country_code": "AR",
            "timezone": "America/Argentina/Buenos_Aires"
        }
    },
    "transaction_id": "a1b2c3d4-e5f6-7890-1234-567890abcdef",
    "status": "COMPLETED",
    "payment_method": "CREDIT",
    "description": "in-store purchase"
}
```

### Access Logs

Generates web access log data. After indexing, `hosts.json` and `clients.json` files will be created.

**Example Document (`access_log`):**
```json
{
    "@timestamp": "2024-10-01T08:59:12.987654",
    "client_id": "client-015",
    "client_ip": "198.51.100.14",
    "username": "johndoe",
    "device_type": "Mobile",
    "host_id": "host-3",
    "host": "host-3.example.com",
    "host_ip": "172.16.10.3",
    "method": "GET",
    "url": "/products/category/electronics",
    "status": 200,
    "bytes_sent": 8192,
    "referrer": "[https://www.google.com/](https://www.google.com/)",
    "user_agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) ...",
    "location": { "lat": 40.7128, "lon": -74.0060 },
    "geo_info": {
        "city": "New York",
        "country_code": "US",
        "timezone": "America/New_York"
    }
}
```

## Project Structure

```
.
├── app.py              # The main Flask web application that serves the UI.
├── script_executor.py  # The script that generates & indexes data into ES.
├── requirements.txt    # The Python dependencies for the project.
└── templates
    └── index.html      # The HTML file for the user interface.
